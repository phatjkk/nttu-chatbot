{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://huggingface.co/vilm/vietcuna-7b-v3?library=transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để kiểm tra nhanh model `vilm/vietcuna-7b-v3`, bạn có thể sử dụng **Transformers pipeline** hoặc tải trực tiếp mô hình và tokenizer từ Hugging Face. Đây là cách thực hiện một bài test đơn giản nhất:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Sử dụng Transformers pipeline**\n",
    "\n",
    "Dùng pipeline để load và test mô hình nhanh:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Tạo pipeline cho text-generation\n",
    "pipe = pipeline(\"text-generation\", model=\"vilm/vietcuna-7b-v3\")\n",
    "\n",
    "# Test model với một prompt\n",
    "prompt = \"Việt Nam là một quốc gia như thế nào?\"\n",
    "output = pipe(prompt, max_length=100, temperature=0.7)\n",
    "\n",
    "# In kết quả\n",
    "print(\"Kết quả từ model:\")\n",
    "print(output[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Load mô hình trực tiếp và test**\n",
    "\n",
    "Nếu muốn có nhiều tùy chỉnh hơn, bạn có thể load mô hình và tokenizer trực tiếp:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer và model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vilm/vietcuna-7b-v3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vilm/vietcuna-7b-v3\")\n",
    "\n",
    "# Prompt input\n",
    "prompt = \"Việt Nam là một quốc gia như thế nào?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output từ model\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Decode kết quả\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Kết quả từ model:\")\n",
    "print(decoded_output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Lưu ý**\n",
    "- **Yêu cầu GPU**: Model `vilm/vietcuna-7b-v3` là mô hình lớn, sử dụng GPU sẽ cải thiện hiệu suất.\n",
    "- **Cài đặt các thư viện cần thiết**:\n",
    "  ```bash\n",
    "  pip install transformers torch\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết quả mong đợi**\n",
    "Mô hình sẽ trả về văn bản tiếp tục dựa trên prompt bạn cung cấp. Ví dụ:\n",
    "```\n",
    "Kết quả từ model:\n",
    "Việt Nam là một quốc gia ở Đông Nam Á, nổi tiếng với nền văn hóa đa dạng và lịch sử lâu đời...\n",
    "```\n",
    "\n",
    "Nếu bạn gặp bất kỳ lỗi nào, hãy chia sẻ để mình hỗ trợ thêm!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
