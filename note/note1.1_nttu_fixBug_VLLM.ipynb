{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lỗi bạn đang gặp liên quan đến xung đột dependency khi cài đặt các thư viện Python qua `pip`. Dù không nghiêm trọng đến mức làm hỏng toàn bộ hệ thống, nó có thể gây ra vấn đề khi chạy các ứng dụng nếu các dependencies không tương thích. Dưới đây là phân tích và cách giải quyết:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Nguyên nhân**\n",
    "- **Conflicts giữa các phiên bản**: \n",
    "  - Một số thư viện yêu cầu các phiên bản cụ thể của `pydantic` và `protobuf`. Ví dụ:\n",
    "    - `vllm` yêu cầu `pydantic==1.10.13`.\n",
    "    - `langchain` yêu cầu `pydantic>=2.7.4`.\n",
    "    - `tensorflow` yêu cầu `protobuf<5.0.0`, trong khi phiên bản bạn cài đặt là `protobuf 5.28.3`.\n",
    "- **Dependency resolver của pip không kiểm tra toàn bộ cây dependency**: Điều này dẫn đến việc các phiên bản không tương thích cùng tồn tại.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Tác động**\n",
    "- **Ứng dụng có thể không chạy đúng**:\n",
    "  - Nếu bạn sử dụng các thư viện như `langchain` hoặc `vllm`, ứng dụng có thể lỗi khi khởi chạy.\n",
    "- **Khó bảo trì**:\n",
    "  - Về lâu dài, việc sử dụng các dependencies không tương thích có thể gây ra lỗi tiềm ẩn.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Cách giải quyết**\n",
    "Để khắc phục vấn đề, bạn có thể thử các bước sau:\n",
    "\n",
    "#### a. **Tạo môi trường ảo mới**\n",
    "Đảm bảo bạn làm việc trong một môi trường Python sạch:\n",
    "```bash\n",
    "python -m venv myenv\n",
    "source myenv/bin/activate  # Linux/Mac\n",
    "myenv\\Scripts\\activate  # Windows\n",
    "```\n",
    "\n",
    "#### b. **Cài đặt dependencies tuần tự**\n",
    "Cài đặt các phiên bản cụ thể, bắt đầu từ thư viện có yêu cầu nghiêm ngặt nhất:\n",
    "```bash\n",
    "pip install pydantic==1.10.13 protobuf<5.0.0\n",
    "pip install tensorflow\n",
    "pip install langchain[vllm]  # Nếu cần cả hai thư viện này\n",
    "```\n",
    "\n",
    "#### c. **Sử dụng file `requirements.txt`**\n",
    "Tạo file `requirements.txt` với các dependencies chính xác:\n",
    "```txt\n",
    "pydantic==1.10.13\n",
    "protobuf<5.0.0\n",
    "tensorflow\n",
    "langchain\n",
    "vllm\n",
    "```\n",
    "Cài đặt với:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### d. **Kiểm tra phiên bản Python**\n",
    "Đảm bảo bạn sử dụng phiên bản Python tương thích, vì một số xung đột có thể do phiên bản Python không phù hợp.\n",
    "\n",
    "#### e. **Sử dụng công cụ quản lý dependency**\n",
    "Sử dụng `pip-tools` hoặc `poetry` để tự động giải quyết dependencies:\n",
    "```bash\n",
    "pip install pip-tools\n",
    "pip-compile requirements.in\n",
    "pip-sync\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Nếu bạn vẫn gặp lỗi, hãy chia sẻ thêm chi tiết về ứng dụng bạn đang chạy để mình hỗ trợ sâu hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm - Cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "ImportError: Could not import vllm python package. Please install it with `pip install vllm`.\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NOTE: If your import is failing due to a missing package, you can\n",
    "manually install dependencies using either !pip or !apt.\n",
    "\n",
    "To view examples of installing some common dependencies, click the\n",
    "\"Open Examples\" button below.\n",
    "```\n",
    "\n",
    "Nguyên nhân lỗi\n",
    "Lỗi libcudart.so.11.0 không tìm thấy xảy ra dù bạn đã cài đặt CUDA 12.2 vì phiên bản vllm (0.2.2+cu118) yêu cầu CUDA 11.8. Điều này dẫn đến không tương thích giữa phiên bản vllm và CUDA runtime hiện tại của hệ thống.\n",
    "\n",
    "Cụ thể:\n",
    "\n",
    "Phiên bản vllm bạn cài đặt được xây dựng dựa trên CUDA 11.8 (cu118).\n",
    "CUDA 12.2 không tương thích ngược hoàn toàn với CUDA 11.8. Vì vậy, vllm không thể tìm thấy file libcudart.so.11.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên hình ảnh bạn cung cấp:\n",
    "\n",
    "1. **`nvcc --version`:**\n",
    "   - Hiển thị CUDA Compilation Tools đang ở **version 11.8**. Đây là phiên bản được cài đặt và sử dụng bởi trình biên dịch CUDA (CUDA Toolkit).\n",
    "\n",
    "2. **`nvidia-smi`:**\n",
    "   - Hiển thị **CUDA Version: 12.2**. Đây là phiên bản CUDA runtime được tích hợp trong driver NVIDIA (Driver version 535.104.05). \n",
    "\n",
    "### Điều này có nghĩa gì?\n",
    "- Phiên bản CUDA mà bạn đang sử dụng để biên dịch mã (nvcc) là **11.8**.\n",
    "- Phiên bản CUDA runtime được hỗ trợ bởi driver NVIDIA là **12.2**.\n",
    "\n",
    "### Quan trọng:\n",
    "- Các thư viện bạn đã cài đặt (vd. `xformers`, `vllm`) với gói `cu118` sẽ hoạt động với CUDA 11.8, không cần nâng cấp CUDA runtime lên 12.2.\n",
    "- Sự khác biệt này thường không gây vấn đề, vì CUDA runtime có khả năng tương thích ngược (backward compatibility).\n",
    "\n",
    "Nếu cần thêm giải thích, mình sẵn sàng hỗ trợ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nguyên nhân lỗi**\n",
    "- **Lỗi:** `libcudart.so.11.0` không tìm thấy, dẫn đến không thể import `vllm`.\n",
    "- **Nguyên nhân:**\n",
    "  1. `vllm` phiên bản `0.2.2+cu118` yêu cầu thư viện CUDA runtime 11.8 (`libcudart.so.11.0`).\n",
    "  2. Môi trường hiện tại sử dụng CUDA runtime 12.2 (hiển thị qua `nvidia-smi`), không hoàn toàn tương thích ngược với CUDA 11.8.\n",
    "  3. Biến môi trường `LD_LIBRARY_PATH` không trỏ đúng đến thư viện CUDA 11.8.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chi tiết các bước fix**\n",
    "\n",
    "#### **1. Cài đặt CUDA 11.8**\n",
    "Chạy các lệnh sau để tải và cài CUDA 11.8:\n",
    "```bash\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\n",
    "!sudo sh cuda_11.8.0_520.61.05_linux.run --silent --toolkit\n",
    "```\n",
    "- **Output mong muốn:** CUDA 11.8 được cài đặt tại `/usr/local/cuda-11.8`.\n",
    "\n",
    "#### **2. Cập nhật biến môi trường**\n",
    "Đảm bảo biến môi trường `PATH` và `LD_LIBRARY_PATH` trỏ đúng đến CUDA 11.8:\n",
    "```python\n",
    "import os\n",
    "os.environ['PATH'] = '/usr/local/cuda-11.8/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "```\n",
    "\n",
    "#### **3. Tạo liên kết tĩnh cho `libcudart`**\n",
    "Đảm bảo `libcudart.so` trỏ đúng đến phiên bản `libcudart.so.11.8.89`:\n",
    "```bash\n",
    "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcudart.so.11.8.89 /usr/local/cuda-11.8/lib64/libcudart.so\n",
    "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcudart.so.11.8.89 /usr/local/cuda-11.8/lib64/libcudart.so.11.0\n",
    "```\n",
    "\n",
    "#### **4. Kiểm tra thư viện CUDA**\n",
    "Xác minh rằng `libcudart.so` tồn tại và được liên kết đúng:\n",
    "```bash\n",
    "!ls /usr/local/cuda-11.8/lib64 | grep libcudart\n",
    "!ls -l /usr/local/cuda-11.8/lib64/libcudart.so\n",
    "```\n",
    "- **Output mong muốn:**\n",
    "  ```\n",
    "  libcudart.so -> libcudart.so.11.8.89\n",
    "  libcudart.so.11.0\n",
    "  libcudart.so.11.8.89\n",
    "  ```\n",
    "\n",
    "#### **5. Gỡ và cài lại `vllm`**\n",
    "Cài phiên bản `vllm` tương thích với CUDA 11.8:\n",
    "```bash\n",
    "!pip uninstall -y vllm\n",
    "!pip install https://github.com/vllm-project/vllm/releases/download/v0.2.2/vllm-0.2.2+cu118-cp310-cp310-manylinux1_x86_64.whl\n",
    "```\n",
    "- **Output mong muốn:** `vllm` cài thành công.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code kiểm tra**\n",
    "\n",
    "#### **1. Kiểm tra phiên bản CUDA**\n",
    "Chạy các lệnh sau để kiểm tra CUDA runtime và toolkit:\n",
    "```bash\n",
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "```\n",
    "- **Output mong muốn:**\n",
    "  - `nvcc --version` hiển thị `Cuda compilation tools, release 11.8`.\n",
    "  - `nvidia-smi` hiển thị GPU driver hỗ trợ CUDA 12.2 nhưng runtime có thể sử dụng CUDA 11.8.\n",
    "\n",
    "#### **2. Kiểm tra `vllm`**\n",
    "Kiểm tra `vllm` đã cài đúng chưa:\n",
    "```bash\n",
    "!pip show vllm\n",
    "```\n",
    "- **Output mong muốn:** Hiển thị thông tin gói `vllm`, bao gồm `0.2.2+cu118`.\n",
    "\n",
    "#### **3. Chạy ứng dụng**\n",
    "Kiểm tra `vllm` hoạt động:\n",
    "```python\n",
    "from vllm import LLM\n",
    "llm = LLM()\n",
    "```\n",
    "- **Output mong muốn:** Không có lỗi, ứng dụng khởi tạo thành công.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm tắt các lệnh**\n",
    "```bash\n",
    "# Cài đặt CUDA 11.8\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\n",
    "!sudo sh cuda_11.8.0_520.61.05_linux.run --silent --toolkit\n",
    "\n",
    "# Cập nhật biến môi trường\n",
    "import os\n",
    "os.environ['PATH'] = '/usr/local/cuda-11.8/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# Tạo liên kết tĩnh\n",
    "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcudart.so.11.8.89 /usr/local/cuda-11.8/lib64/libcudart.so\n",
    "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcudart.so.11.8.89 /usr/local/cuda-11.8/lib64/libcudart.so.11.0\n",
    "\n",
    "# Kiểm tra thư viện\n",
    "!ls /usr/local/cuda-11.8/lib64 | grep libcudart\n",
    "!ls -l /usr/local/cuda-11.8/lib64/libcudart.so\n",
    "\n",
    "# Cài lại vllm\n",
    "!pip uninstall -y vllm\n",
    "!pip install https://github.com/vllm-project/vllm/releases/download/v0.2.2/vllm-0.2.2+cu118-cp310-cp310-manylinux1_x86_64.whl\n",
    "\n",
    "# Kiểm tra\n",
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "!pip show vllm\n",
    "\n",
    "# Kiểm tra vllm\n",
    "from vllm import LLM\n",
    "llm = LLM()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  !pip -qq install -U xformers --index-url https://download.pytorch.org/whl/cu118 là sao thay cho \n",
    "```\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\n",
    "!sudo sh cuda_11.8.0_520.61.05_linux.run --silent --toolkit\n",
    "```\n",
    "\n",
    "Được ko nhỉ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn **không thể thay thế** lệnh cài đặt `CUDA Toolkit` từ nguồn chính thức của NVIDIA:\n",
    "\n",
    "```bash\n",
    "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\n",
    "!sudo sh cuda_11.8.0_520.61.05_linux.run --silent --toolkit\n",
    "```\n",
    "\n",
    "bằng lệnh cài đặt `xformers`:\n",
    "\n",
    "```bash\n",
    "!pip -qq install -U xformers --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "### **Lý do**\n",
    "1. **`CUDA Toolkit` và `xformers` là hai thứ khác nhau:**\n",
    "   - `CUDA Toolkit`: Bao gồm các thư viện cấp thấp như `libcudart.so` và các công cụ cần thiết để làm việc với GPU.\n",
    "   - `xformers`: Chỉ là một thư viện Python được tối ưu hóa để sử dụng CUDA runtime (phụ thuộc vào `CUDA Toolkit` đã được cài đặt trước đó).\n",
    "\n",
    "2. **`xformers` không tự cài đặt CUDA Toolkit:**\n",
    "   - Lệnh `pip install xformers` chỉ cài thư viện Python, nhưng nó yêu cầu hệ thống đã có sẵn `CUDA Toolkit` để hoạt động.\n",
    "   - Nếu `CUDA Toolkit` chưa được cài, việc cài `xformers` vẫn sẽ gặp lỗi thiếu `libcudart.so`.\n",
    "\n",
    "3. **Phụ thuộc vào môi trường Colab:**\n",
    "   - Trên Google Colab, đôi khi CUDA Toolkit được cài sẵn, nhưng không phải lúc nào cũng là phiên bản bạn cần (ví dụ, Colab có thể cài CUDA 12.2 trong khi bạn cần CUDA 11.8).\n",
    "\n",
    "### **Cách thay thế hiệu quả**\n",
    "Nếu bạn muốn cài đặt CUDA runtime mà không dùng bộ cài `.run` của NVIDIA, bạn có thể thử sử dụng gói `conda` hoặc `pip` để cài đặt các thư viện CUDA runtime. Tuy nhiên, với Google Colab, cách tốt nhất vẫn là sử dụng bộ cài từ NVIDIA.\n",
    "\n",
    "Nếu bạn muốn thử giải pháp nhẹ hơn, hãy làm như sau:\n",
    "\n",
    "#### **Cách thay thế nhẹ hơn để cài đặt runtime cần thiết:**\n",
    "Thay vì dùng bộ cài NVIDIA, bạn có thể sử dụng `pip` để tải các thư viện cần thiết:\n",
    "\n",
    "```bash\n",
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-cuda-runtime-cu11==11.8.89\n",
    "!pip install nvidia-cuda-cupti-cu11==11.8.89\n",
    "!pip install nvidia-cudnn-cu11==8.6.0.163\n",
    "```\n",
    "\n",
    "### **Tóm lại**\n",
    "- Nếu bạn cần `CUDA Toolkit` đầy đủ, **không thể bỏ qua** lệnh tải `.run` từ NVIDIA.\n",
    "- Nếu chỉ cần CUDA runtime cho các thư viện Python như `xformers` hoặc `vllm`, bạn có thể sử dụng `pip` để tải các runtime cần thiết như trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up vllm lên 1 phiên bản cudat12.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip -qq install -U xformers --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -qq install langchain optimum qdrant-client wikipedia FastAPI uvicorn pyngrok\n",
    "!pip install --upgrade pydantic\n",
    "# !pip -qq install https://github.com/vllm-project/vllm/releases/download/v0.2.2/vllm-0.2.2+cu118-cp310-cp310-manylinux1_x86_64.whl\n",
    "# !pip install vllm\n",
    "\n",
    "# 3. Cài đặt lại vllm với phiên bản phù hợp CUDA 12.2\n",
    "!pip install https://github.com/vllm-project/vllm/releases/download/v0.2.2/vllm-0.2.2+cu121-cp310-cp310-manylinux1_x86_64.whl\n",
    "!pip install vllm\n",
    "# 4. Kiểm tra lại libcudart và vllm\n",
    "!ls /usr/lib64-nvidia | grep libcudart\n",
    "!pip show vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bug Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ nội dung trong notebook của bạn, mô hình `phatjk/vietcuna-7b-v3-AWQ` được tải bằng cách chỉ định nó trong các cấu hình và thông qua class `LLMServe`. Đây là cách mà mô hình đang được xử lý:\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách mô hình đang được tải**\n",
    "\n",
    "#### **1. Cấu hình mô hình**\n",
    "Mô hình `phatjk/vietcuna-7b-v3-AWQ` được cấu hình thông qua biến toàn cục:\n",
    "```python\n",
    "GENERATE_MODEL_NAME = \"phatjk/vietcuna-7b-v3-AWQ\"\n",
    "```\n",
    "\n",
    "#### **2. Phương thức `load_model_pipeline`**\n",
    "Mô hình được khởi tạo trong phương thức `load_model_pipeline` bằng cách sử dụng `VLLM`:\n",
    "```python\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    llm = VLLM(\n",
    "        model=GENERATE_MODEL_NAME,  # Tên mô hình từ Hugging Face\n",
    "        trust_remote_code=True,    # Cho phép sử dụng mã tùy chỉnh từ repo mô hình\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        temperature=0.4,\n",
    "        dtype=\"half\",              # Sử dụng kiểu số học `half` (FP16) để tối ưu GPU\n",
    "        vllm_kwargs={\"quantization\": \"awq\"}  # Sử dụng `awq` để lượng tử hóa\n",
    "    )\n",
    "    return llm\n",
    "```\n",
    "\n",
    "- **Chi tiết quan trọng:**\n",
    "  - `model`: Sử dụng mô hình từ Hugging Face với ID `phatjk/vietcuna-7b-v3-AWQ`.\n",
    "  - `trust_remote_code`: Cần thiết nếu mô hình có mã nguồn tùy chỉnh (custom).\n",
    "  - `quantization`: Lựa chọn lượng tử hóa (AWQ - Activation Weight Quantization) để giảm tải bộ nhớ GPU.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Thư viện `transformers`**\n",
    "Phương thức `load_model_pipeline` dựa trên mô hình và mã nguồn từ Hugging Face. Nó sử dụng thư viện `transformers` trong nền để tải mô hình.\n",
    "\n",
    "#### **4. Vấn đề với mô hình**\n",
    "Nếu bạn gặp lỗi liên quan đến `KeyError` khi tải trọng số, có thể là do:\n",
    "1. **Không tương thích trọng số (`weights`) với kiến trúc `VLLM`**.\n",
    "2. **Mô hình cần mã nguồn tùy chỉnh không được tải đúng.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách kiểm tra và sửa lỗi**\n",
    "\n",
    "#### **1. Kiểm tra tải mô hình trực tiếp từ Hugging Face**\n",
    "Trước tiên, kiểm tra xem mô hình có tải được trực tiếp từ Hugging Face không:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"phatjk/vietcuna-7b-v3-AWQ\"\n",
    "\n",
    "# Tải mô hình và tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "```\n",
    "\n",
    "Nếu lỗi xảy ra, mô hình trên Hugging Face có thể không tương thích.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Thử mô hình thay thế**\n",
    "Nếu `phatjk/vietcuna-7b-v3-AWQ` không hoạt động, thử tải mô hình khác từ Hugging Face:\n",
    "```python\n",
    "GENERATE_MODEL_NAME = \"facebook/opt-1.3b\"  # Ví dụ mô hình thay thế\n",
    "\n",
    "app = LLMServe()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Tải mô hình đầy đủ (nếu lỗi do thiếu trọng số)**\n",
    "Sử dụng `huggingface_hub` để tải toàn bộ mô hình:\n",
    "```bash\n",
    "!pip install huggingface_hub\n",
    "```\n",
    "\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Tải mô hình về thư mục local\n",
    "snapshot_download(\n",
    "    repo_id=\"phatjk/vietcuna-7b-v3-AWQ\",\n",
    "    local_dir=\"/content/phatjk-vietcuna-7b-v3-AWQ\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết luận**\n",
    "- Mô hình đang được tải qua Hugging Face với cấu hình từ `VLLM`.\n",
    "- Lỗi có thể do trọng số hoặc không tương thích với `VLLM`. Kiểm tra trực tiếp từ Hugging Face hoặc thử mô hình thay thế sẽ giúp xác định nguyên nhân."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ nội dung trong notebook của bạn, mô hình `phatjk/vietcuna-7b-v3-AWQ` được tải bằng cách chỉ định nó trong các cấu hình và thông qua class `LLMServe`. Đây là cách mà mô hình đang được xử lý:\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách mô hình đang được tải**\n",
    "\n",
    "#### **1. Cấu hình mô hình**\n",
    "Mô hình `phatjk/vietcuna-7b-v3-AWQ` được cấu hình thông qua biến toàn cục:\n",
    "```python\n",
    "GENERATE_MODEL_NAME = \"phatjk/vietcuna-7b-v3-AWQ\"\n",
    "```\n",
    "\n",
    "#### **2. Phương thức `load_model_pipeline`**\n",
    "Mô hình được khởi tạo trong phương thức `load_model_pipeline` bằng cách sử dụng `VLLM`:\n",
    "```python\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    llm = VLLM(\n",
    "        model=GENERATE_MODEL_NAME,  # Tên mô hình từ Hugging Face\n",
    "        trust_remote_code=True,    # Cho phép sử dụng mã tùy chỉnh từ repo mô hình\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        temperature=0.4,\n",
    "        dtype=\"half\",              # Sử dụng kiểu số học `half` (FP16) để tối ưu GPU\n",
    "        vllm_kwargs={\"quantization\": \"awq\"}  # Sử dụng `awq` để lượng tử hóa\n",
    "    )\n",
    "    return llm\n",
    "```\n",
    "\n",
    "- **Chi tiết quan trọng:**\n",
    "  - `model`: Sử dụng mô hình từ Hugging Face với ID `phatjk/vietcuna-7b-v3-AWQ`.\n",
    "  - `trust_remote_code`: Cần thiết nếu mô hình có mã nguồn tùy chỉnh (custom).\n",
    "  - `quantization`: Lựa chọn lượng tử hóa (AWQ - Activation Weight Quantization) để giảm tải bộ nhớ GPU.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Thư viện `transformers`**\n",
    "Phương thức `load_model_pipeline` dựa trên mô hình và mã nguồn từ Hugging Face. Nó sử dụng thư viện `transformers` trong nền để tải mô hình.\n",
    "\n",
    "#### **4. Vấn đề với mô hình**\n",
    "Nếu bạn gặp lỗi liên quan đến `KeyError` khi tải trọng số, có thể là do:\n",
    "1. **Không tương thích trọng số (`weights`) với kiến trúc `VLLM`**.\n",
    "2. **Mô hình cần mã nguồn tùy chỉnh không được tải đúng.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách kiểm tra và sửa lỗi**\n",
    "\n",
    "#### **1. Kiểm tra tải mô hình trực tiếp từ Hugging Face**\n",
    "Trước tiên, kiểm tra xem mô hình có tải được trực tiếp từ Hugging Face không:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"phatjk/vietcuna-7b-v3-AWQ\"\n",
    "\n",
    "# Tải mô hình và tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "```\n",
    "\n",
    "Nếu lỗi xảy ra, mô hình trên Hugging Face có thể không tương thích.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Thử mô hình thay thế**\n",
    "Nếu `phatjk/vietcuna-7b-v3-AWQ` không hoạt động, thử tải mô hình khác từ Hugging Face:\n",
    "```python\n",
    "GENERATE_MODEL_NAME = \"facebook/opt-1.3b\"  # Ví dụ mô hình thay thế\n",
    "\n",
    "app = LLMServe()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Tải mô hình đầy đủ (nếu lỗi do thiếu trọng số)**\n",
    "Sử dụng `huggingface_hub` để tải toàn bộ mô hình:\n",
    "```bash\n",
    "!pip install huggingface_hub\n",
    "```\n",
    "\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Tải mô hình về thư mục local\n",
    "snapshot_download(\n",
    "    repo_id=\"phatjk/vietcuna-7b-v3-AWQ\",\n",
    "    local_dir=\"/content/phatjk-vietcuna-7b-v3-AWQ\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết luận**\n",
    "- Mô hình đang được tải qua Hugging Face với cấu hình từ `VLLM`.\n",
    "- Lỗi có thể do trọng số hoặc không tương thích với `VLLM`. Kiểm tra trực tiếp từ Hugging Face hoặc thử mô hình thay thế sẽ giúp xác định nguyên nhân."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLLM là gì? thay bằng cái khác được ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Giải thích phương thức `load_model_pipeline`**\n",
    "\n",
    "Phương thức `load_model_pipeline` chịu trách nhiệm khởi tạo một mô hình ngôn ngữ lớn (LLM) bằng cách sử dụng thư viện **`VLLM`**. Đây là cách mà mô hình được tải và cấu hình để thực hiện các tác vụ ngôn ngữ tự nhiên (NLP) như trả lời câu hỏi, tạo văn bản, hoặc hỗ trợ các pipeline khác.\n",
    "\n",
    "#### **Chi tiết về phương thức**\n",
    "```python\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    llm = VLLM(\n",
    "        model=GENERATE_MODEL_NAME,  # Tên mô hình cần tải, từ Hugging Face hoặc đường dẫn cục bộ.\n",
    "        trust_remote_code=True,    # Cho phép sử dụng mã nguồn tùy chỉnh của mô hình.\n",
    "        max_new_tokens=max_new_tokens,  # Số token tối đa cho mỗi kết quả tạo văn bản.\n",
    "        top_k=10,                  # Số lượng token tốt nhất trong không gian tìm kiếm.\n",
    "        top_p=0.95,                # Nút mềm cho sampling (nucleus sampling).\n",
    "        temperature=0.4,           # Điều chỉnh độ ngẫu nhiên trong kết quả sinh.\n",
    "        dtype=\"half\",              # Sử dụng kiểu số học `half` (FP16) để tối ưu GPU.\n",
    "        vllm_kwargs={\"quantization\": \"awq\"}  # Dùng lượng tử hóa để giảm tải bộ nhớ.\n",
    "    )\n",
    "    return llm\n",
    "```\n",
    "\n",
    "### **Mục đích của phương thức**\n",
    "1. **Tải mô hình một cách chính xác:**\n",
    "   - Thay vì tự viết mã để khởi tạo và cấu hình mô hình, phương thức này tận dụng thư viện `VLLM` để đảm bảo quá trình tải và sử dụng mô hình được thực hiện đúng cách.\n",
    "   - Điều này đặc biệt quan trọng nếu mô hình có mã nguồn tùy chỉnh (custom logic), yêu cầu `trust_remote_code=True`.\n",
    "\n",
    "2. **Tối ưu tài nguyên GPU:**\n",
    "   - Sử dụng kiểu dữ liệu `dtype=\"half\"` và lượng tử hóa `awq` để giảm mức sử dụng bộ nhớ GPU trong khi vẫn giữ được độ chính xác của mô hình.\n",
    "   - Các tham số như `top_k`, `top_p`, và `temperature` giúp tối ưu hóa hiệu suất sinh văn bản.\n",
    "\n",
    "3. **Chuẩn hóa quá trình khởi tạo:**\n",
    "   - Phương thức này chuẩn hóa việc khởi tạo mô hình, đảm bảo rằng tất cả các pipeline khác có thể sử dụng mô hình theo một cách thống nhất.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tại sao phải tải qua phương thức này?**\n",
    "\n",
    "#### **1. Dễ bảo trì và mở rộng**\n",
    "- Nếu sau này bạn muốn thay đổi cách cấu hình mô hình (ví dụ: đổi mô hình khác, thêm các tham số), bạn chỉ cần sửa một nơi duy nhất, thay vì phải thay đổi toàn bộ các đoạn mã sử dụng mô hình.\n",
    "\n",
    "#### **2. Tích hợp chặt chẽ với các pipeline khác**\n",
    "- Phương thức này được gọi từ lớp `LLMServe`, nơi nó được tích hợp với các pipeline khác như **Retrieval-Augmented Generation (RAG)**. Điều này giúp quản lý mô hình và quy trình một cách tập trung.\n",
    "\n",
    "#### **3. Hỗ trợ nhiều loại mô hình**\n",
    "- Bằng cách sử dụng `VLLM`, bạn có thể linh hoạt tải các mô hình từ Hugging Face hoặc các mô hình được tối ưu hóa với lượng tử hóa. Nếu bạn cần tải mô hình khác, chỉ cần thay đổi giá trị `GENERATE_MODEL_NAME`.\n",
    "\n",
    "#### **4. Hỗ trợ tùy chỉnh:**\n",
    "- Các tham số như `top_k`, `top_p`, và `temperature` có thể dễ dàng điều chỉnh để phù hợp với mục đích sử dụng, như:\n",
    "  - Sinh văn bản sáng tạo: Dùng `temperature` cao hơn (ví dụ: 0.8).\n",
    "  - Sinh văn bản chính xác: Dùng `temperature` thấp hơn (ví dụ: 0.2).\n",
    "\n",
    "---\n",
    "\n",
    "### **So sánh với cách tải mô hình trực tiếp**\n",
    "Nếu không dùng phương thức này, bạn có thể tự tải mô hình bằng `VLLM` hoặc thư viện `transformers`. Tuy nhiên:\n",
    "- **Sẽ thiếu kiểm soát:** Bạn phải tự thêm các tham số tối ưu hóa (FP16, lượng tử hóa) và cấu hình khác.\n",
    "- **Khó bảo trì:** Mọi pipeline cần mô hình đều phải lặp lại mã khởi tạo.\n",
    "- **Không hỗ trợ mã nguồn tùy chỉnh:** Bạn cần tự viết mã để xử lý các mô hình có logic đặc thù (custom logic).\n",
    "\n",
    "---\n",
    "\n",
    "### **Khi nào không cần dùng `load_model_pipeline`?**\n",
    "- Nếu bạn chỉ cần thử nghiệm nhanh một mô hình đơn giản, bạn có thể bỏ qua phương thức này và tải mô hình trực tiếp:\n",
    "  ```python\n",
    "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "  model_name = \"phatjk/vietcuna-7b-v3-AWQ\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kết luận**\n",
    "Phương thức `load_model_pipeline` giúp:\n",
    "1. Chuẩn hóa và tối ưu hóa quá trình tải mô hình.\n",
    "2. Dễ dàng tích hợp vào các pipeline phức tạp.\n",
    "3. Hỗ trợ tùy chỉnh linh hoạt các tham số.\n",
    "\n",
    "Bạn nên giữ phương thức này để đảm bảo tính ổn định và dễ bảo trì của hệ thống, đặc biệt khi làm việc với các mô hình lớn và phức tạp như `phatjk/vietcuna-7b-v3-AWQ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Thay thế `VLLM` bằng phương thức khác**\n",
    "\n",
    "Nếu bạn không muốn sử dụng `VLLM`, có thể thay thế bằng các framework hoặc thư viện khác như `transformers` hoặc `HuggingFacePipeline`. Điều này tùy thuộc vào mục đích sử dụng và môi trường triển khai của bạn.\n",
    "\n",
    "Dưới đây là các lựa chọn thay thế:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Thay `VLLM` bằng `transformers`**\n",
    "\n",
    "#### **Ưu điểm**\n",
    "- Dễ sử dụng và phổ biến.\n",
    "- Hỗ trợ hầu hết các mô hình trên Hugging Face.\n",
    "- Không cần thêm `trust_remote_code` nếu không sử dụng mã tùy chỉnh.\n",
    "\n",
    "#### **Cách thực hiện**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    # Tải mô hình và tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        GENERATE_MODEL_NAME,\n",
    "        torch_dtype=\"auto\",  # Tự động chọn FP16 nếu GPU hỗ trợ\n",
    "        device_map=\"auto\"    # Tự động ánh xạ mô hình lên GPU\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GENERATE_MODEL_NAME)\n",
    "    \n",
    "    # Tạo pipeline\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "    return pipe\n",
    "```\n",
    "\n",
    "#### **Lợi ích**\n",
    "- Không cần cài đặt thêm thư viện như `VLLM`.\n",
    "- Hỗ trợ pipeline cho nhiều tác vụ khác nhau: `text-generation`, `question-answering`, v.v.\n",
    "\n",
    "#### **Hạn chế**\n",
    "- Ít tối ưu hơn `VLLM` về tốc độ sinh văn bản trên các mô hình lớn.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Thay bằng `HuggingFacePipeline`**\n",
    "\n",
    "#### **Ưu điểm**\n",
    "- Là một thành phần tích hợp của LangChain.\n",
    "- Dễ dàng tích hợp với các pipeline như Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "#### **Cách thực hiện**\n",
    "```python\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    # Tải mô hình và tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        GENERATE_MODEL_NAME,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GENERATE_MODEL_NAME)\n",
    "    \n",
    "    # Tạo pipeline Hugging Face\n",
    "    hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Tích hợp với LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "    return llm\n",
    "```\n",
    "\n",
    "#### **Lợi ích**\n",
    "- Kết hợp tốt với các thành phần khác của LangChain như PromptTemplate hoặc Retriever.\n",
    "- Cung cấp giao diện thống nhất cho RAG.\n",
    "\n",
    "#### **Hạn chế**\n",
    "- Không tối ưu hiệu năng so với `VLLM`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Thay bằng `Hugging Face Inference API`**\n",
    "\n",
    "#### **Ưu điểm**\n",
    "- Không cần GPU cục bộ, chạy trên đám mây Hugging Face.\n",
    "- Thích hợp nếu bạn không có tài nguyên GPU.\n",
    "\n",
    "#### **Cách thực hiện**\n",
    "```python\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=GENERATE_MODEL_NAME,\n",
    "        model_kwargs={\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "        huggingfacehub_api_token=HUGGINGFACE_API_KEY\n",
    "    )\n",
    "    return llm\n",
    "```\n",
    "\n",
    "#### **Lợi ích**\n",
    "- Không yêu cầu tài nguyên GPU cục bộ.\n",
    "- Dễ dàng sử dụng và quản lý qua API của Hugging Face.\n",
    "\n",
    "#### **Hạn chế**\n",
    "- Tốn chi phí API nếu sử dụng với khối lượng lớn.\n",
    "- Phụ thuộc vào tốc độ mạng.\n",
    "\n",
    "---\n",
    "\n",
    "### **So sánh các phương pháp**\n",
    "\n",
    "| **Phương pháp**                  | **Ưu điểm**                                              | **Hạn chế**                                     |\n",
    "|-----------------------------------|---------------------------------------------------------|-------------------------------------------------|\n",
    "| `VLLM`                           | Tối ưu tốc độ, hỗ trợ lượng tử hóa (quantization).       | Cần cài đặt thêm thư viện, lỗi không tương thích. |\n",
    "| `transformers`                   | Dễ sử dụng, phổ biến.                                   | Tốn nhiều tài nguyên nếu không tối ưu.         |\n",
    "| `HuggingFacePipeline`            | Tích hợp tốt với LangChain.                             | Không tối ưu hiệu năng.                        |\n",
    "| `Hugging Face Inference API`     | Không yêu cầu GPU cục bộ.                               | Tốn chi phí API và phụ thuộc vào tốc độ mạng.  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Khi nào nên thay thế?**\n",
    "- **Dùng `transformers`:** Khi bạn muốn kiểm soát trực tiếp việc tải mô hình.\n",
    "- **Dùng `HuggingFacePipeline`:** Khi bạn tích hợp với LangChain và không cần tối ưu hiệu năng.\n",
    "- **Dùng `Hugging Face Inference API`:** Khi bạn không có GPU hoặc muốn đơn giản hóa triển khai.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cách thử nghiệm**\n",
    "Thay đổi phương thức `load_model_pipeline` trong class `LLMServe` theo một trong các cách trên. Sau đó, kiểm tra khả năng hoạt động:\n",
    "```python\n",
    "app = LLMServe()\n",
    "response = app.rag(\"wiki\").run({\"query\": \"Việt Nam nằm ở đâu trên bản đồ thế giới?\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "Nếu bạn muốn thay đổi hoặc thử nghiệm mô hình khác, mình có thể hỗ trợ thêm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để thay thế `VLLM` bằng `transformers` hoặc `HuggingFacePipeline` mà không làm thay đổi cấu hình và cách tích hợp của class `LLMServe`, chúng ta cần đảm bảo các tham số như `max_new_tokens`, `top_k`, `top_p`, `temperature`, và `dtype` vẫn được áp dụng. \n",
    "\n",
    "Dưới đây là cách thực hiện:\n",
    "\n",
    "---\n",
    "\n",
    "### **Thay bằng `transformers`**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    # Tải mô hình và tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        GENERATE_MODEL_NAME,\n",
    "        torch_dtype=\"auto\",  # Tự động chọn kiểu dữ liệu FP16 nếu GPU hỗ trợ\n",
    "        device_map=\"auto\"    # Tự động ánh xạ mô hình lên GPU\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GENERATE_MODEL_NAME)\n",
    "    \n",
    "    # Tạo pipeline với cấu hình tương tự VLLM\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_k=10,            # Số lượng token tốt nhất\n",
    "        top_p=0.95,          # Nút mềm cho sampling\n",
    "        temperature=0.4      # Điều chỉnh độ ngẫu nhiên\n",
    "    )\n",
    "    return pipe\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Thay bằng `HuggingFacePipeline`**\n",
    "```python\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def load_model_pipeline(self, max_new_tokens=100):\n",
    "    # Tải mô hình và tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        GENERATE_MODEL_NAME,\n",
    "        torch_dtype=\"auto\",  # Tự động chọn kiểu dữ liệu FP16 nếu GPU hỗ trợ\n",
    "        device_map=\"auto\"    # Tự động ánh xạ mô hình lên GPU\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GENERATE_MODEL_NAME)\n",
    "    \n",
    "    # Tạo pipeline từ transformers\n",
    "    hf_pipeline = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        temperature=0.4\n",
    "    )\n",
    "    \n",
    "    # Tích hợp với LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "    return llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Bảo toàn cấu hình hiện tại**\n",
    "- **Tham số tương tự `VLLM`:**\n",
    "  - `max_new_tokens`: Số token tối đa cho mỗi lần sinh văn bản.\n",
    "  - `top_k`, `top_p`, và `temperature`: Được chuyển trực tiếp vào `pipeline` của `transformers`.\n",
    "  - `dtype=\"half\"`: Được thay bằng `torch_dtype=\"auto\"` để tận dụng FP16 nếu GPU hỗ trợ.\n",
    "\n",
    "- **Không thay đổi cách sử dụng trong class `LLMServe`:**\n",
    "  - Chỉ cần thay đổi nội dung của `load_model_pipeline`, các phần còn lại của class vẫn hoạt động bình thường, vì `HuggingFacePipeline` hoặc `transformers.pipeline` trả về một đối tượng có giao diện tương thích.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tích hợp đầy đủ vào `LLMServe`**\n",
    "Cập nhật phương thức `load_model_pipeline` trong class `LLMServe`:\n",
    "\n",
    "```python\n",
    "class LLMServe:\n",
    "    # Các phương thức khác giữ nguyên...\n",
    "\n",
    "    def load_model_pipeline(self, max_new_tokens=100):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "        # Tải mô hình và tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            GENERATE_MODEL_NAME,\n",
    "            torch_dtype=\"auto\",  # Sử dụng FP16 nếu có GPU\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(GENERATE_MODEL_NAME)\n",
    "        \n",
    "        # Tạo pipeline từ transformers\n",
    "        hf_pipeline = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            temperature=0.4\n",
    "        )\n",
    "        \n",
    "        # Tích hợp với LangChain\n",
    "        llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "        return llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kiểm tra hoạt động**\n",
    "Chạy đoạn mã kiểm tra:\n",
    "\n",
    "```python\n",
    "app = LLMServe()\n",
    "\n",
    "# Sử dụng RAG pipeline\n",
    "response = app.rag(\"wiki\").run({\"query\": \"Việt Nam nằm ở đâu trên bản đồ thế giới?\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Khi nào nên chọn cách thay thế này?**\n",
    "1. **Dùng `transformers`:** Khi muốn trực tiếp kiểm soát mô hình và giảm phụ thuộc vào LangChain.\n",
    "2. **Dùng `HuggingFacePipeline`:** Khi muốn tích hợp chặt chẽ với các thành phần của LangChain mà không cần `VLLM`.\n",
    "\n",
    "Nếu cần thêm hỗ trợ hoặc bạn gặp lỗi trong quá trình tích hợp, cứ báo nhé!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lỗi này xảy ra vì mô hình bạn đang sử dụng (`phatjk/vietcuna-7b-v3-AWQ`) được lượng tử hóa bằng phương pháp **AWQ (Activation Weight Quantization)**, nhưng thư viện **`autoawq`** chưa được cài đặt.\n",
    "\n",
    "---\n",
    "\n",
    "### **Giải pháp**\n",
    "\n",
    "#### **1. Cài đặt thư viện `autoawq`**\n",
    "Chạy lệnh sau để cài đặt `autoawq`:\n",
    "\n",
    "```bash\n",
    "!pip install autoawq\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Cài đặt thêm `accelerate` nếu cần**\n",
    "Trong một số trường hợp, `autoawq` cũng yêu cầu thư viện `accelerate` để hỗ trợ tải mô hình. Cài đặt như sau:\n",
    "\n",
    "```bash\n",
    "!pip install accelerate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Kiểm tra lại**\n",
    "Sau khi cài đặt xong, thử khởi tạo lại ứng dụng:\n",
    "\n",
    "```python\n",
    "app = LLMServe()\n",
    "response = app.rag(\"wiki\").run({\"query\": \"Việt Nam nằm ở đâu trên bản đồ thế giới?\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Giải thích**\n",
    "- **AWQ (Activation Weight Quantization):** Là phương pháp lượng tử hóa giảm tải bộ nhớ và tối ưu GPU. Mô hình của bạn (`phatjk/vietcuna-7b-v3-AWQ`) đã được lượng tử hóa bằng phương pháp này, nên cần thư viện `autoawq` để hoạt động.\n",
    "\n",
    "- **`autoawq` và `accelerate`:** Đây là các thư viện cần thiết để hỗ trợ việc tải và thực thi mô hình lượng tử hóa.\n",
    "\n",
    "---\n",
    "\n",
    "Nếu sau khi cài đặt vẫn gặp lỗi, hãy cho mình biết thêm chi tiết để hỗ trợ nhé!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
